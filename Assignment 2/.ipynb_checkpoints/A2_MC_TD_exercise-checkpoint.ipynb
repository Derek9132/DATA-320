{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Dependencies. Make sure all of them are downloaded before the tutoria!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\derek\\anaconda3\\lib\\site-packages (0.27.0)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from gymnasium) (0.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from gymnasium) (4.5.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from gymnasium) (4.11.3)\n",
      "Requirement already satisfied: shimmy<1.0,>=0.1.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from gymnasium) (0.2.1)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from gymnasium) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from gymnasium) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.7.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\derek\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from matplotlib) (1.22.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\derek\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium \n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the followings lines to check if installation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWnjtVG93qMN"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gymnasium as gym\n",
    "import sys\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plotting\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUOCCpuzgH0o"
   },
   "source": [
    "# Monte Carlo and Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPCNhJKl7KZO"
   },
   "outputs": [],
   "source": [
    "# create a sample policy\n",
    "def sample_policy(observation):\n",
    "    \"\"\"\n",
    "    A policy that sticks if the player score is >= 17 and hits otherwise.\n",
    "    \"\"\"\n",
    "    score, dealer_score, usable_ace = observation\n",
    "    return 0 if score >= 17 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 1: complete the following code for MC prediction (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction(policy, env, num_episodes, discount_factor=1.0, max_steps_per_episode=100):\n",
    "    \"\"\"\n",
    "    Monte Carlo prediction algorithm. Calculates the value function\n",
    "    for a given policy using sampling.\n",
    "    \n",
    "    Args:\n",
    "        policy: A function that maps an observation to action probabilities.\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary that maps from state -> value.\n",
    "        The state is a tuple and the value is a float.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Repeat forever:\n",
    "        Generate an episode using pi\n",
    "        For each state s appearing in the episode\n",
    "         G <- return following the first occurrence of s\n",
    "         Append G to return(s)\n",
    "         V(s) = average of return(s)\n",
    "         \n",
    "    Source - Reinforcement Learning: An Introduction by Richard S Sutton and Andrew G Barto\n",
    "    \"\"\"\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum   = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final value function to return\n",
    "    V = defaultdict(float)\n",
    "    \n",
    "    for i_episode in range(num_episodes): # iterate through episodes\n",
    "        \"\"\" IMPLEMENT THIS \"\"\"\n",
    "        G = 0\n",
    "        state, _ = env.reset()\n",
    "        episode = []\n",
    "        for j in range(max_steps_per_episode): # for each state appearing in episode:\n",
    "            action = policy(state) # get action at a state using policy\n",
    "            next_state, reward, done, _, info = env.step(action) # given the action -> set the next state, reward, and whether it is a terminal state\n",
    "            episode.append((state, action, reward)) # add the state and its action and reward to episode\n",
    "            if done: # if terminal state, break\n",
    "                break\n",
    "            state = next_state\n",
    "        for e in episode: \n",
    "            state = e[0]\n",
    "            action = e[1]\n",
    "            reward = e[2]\n",
    "            \n",
    "            first_occurrence = next(i for i, x in enumerate(episode) if x[0] == state) # first occurrence of state s\n",
    "            \n",
    "            G = sum([x[2]*(discount_factor**i) for i, x in enumerate(episode[first_occurrence:])]) # return following first occurrence of s\n",
    "            returns[state] += G # append G to return(s)\n",
    "            returns_count[state] += 1.0\n",
    "            \n",
    "            V[state] = returns[state] / returns_count[state] # value function at that state is average of returns   \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2321
    },
    "colab_type": "code",
    "id": "_BzGgGsq7NYe",
    "outputId": "8c978ca0-8e64-44bd-8844-0ab70b4c5232"
   },
   "outputs": [],
   "source": [
    "blackjack_env = gym.make('Blackjack-v1')\n",
    "#V_10k = mc_prediction(sample_policy, blackjack_env, num_episodes=10000)\n",
    "#V_10k = mc_prediction(sample_policy, blackjack_env, num_episodes=20000)\n",
    "#V_10k = mc_prediction(sample_policy, blackjack_env, num_episodes=30000)\n",
    "V_10k = mc_prediction(sample_policy, blackjack_env, num_episodes=90000)\n",
    "plotting.plot_value_function(V_10k, title=\"90,000 Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your algorithm seems to run correctly, run it longer to see what the real value prediction looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_500k = mc_prediction(sample_policy, blackjack_env, num_episodes=500000)\n",
    "plotting.plot_value_function(V_500k, title=\"500,000 Steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yTVeDmxE_PEy"
   },
   "source": [
    "As expected, the more episode you run, the better your estimate gets. This is shown by the plots being much smoother for $t=500000$ vs $t=10000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## MC Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nku_kyo0-XN-"
   },
   "source": [
    "It's important to break ties arbitrarily when doing control. This is especially important when you initialize all $Q$ or $V$ array to all 0s. If you don't break ties arbitrarily, you will end up always choosing the same action!. Here is a ** argmax ** function that break ties randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(numpy_array):\n",
    "    \"\"\" argmax implementation that chooses randomly between ties \"\"\"\n",
    "    max_indices = np.where(numpy_array == numpy_array.max())[0]\n",
    "    return max_indices[np.random.randint(max_indices.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also providing you with the following function: Given a $Q$ dictionnary and $\\epsilon$, it returns a $\\epsilon$-greedy policy. Also, since the argument $Q$ is a python object, the returned $\\epsilon$-greedy policy will automatically update as you change the $Q$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4fPdsKA-g72"
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 2: complete the following code for MC control (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytvWvkru-lZh"
   },
   "outputs": [],
   "source": [
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1, max_steps_per_episode=100):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Repeat forever:\n",
    "        Generate episode using pi\n",
    "        For each pair (s,a) appearing in episode:\n",
    "            G <- return following the first occurrence of (s,a)\n",
    "            append G to returns(s,a)\n",
    "            Q(s,a) <- average of returns(s,a)\n",
    "        For each s in episode:\n",
    "            a* = argmax_a(Q(s,a))\n",
    "            For all a in A(s):\n",
    "                policy(a|s) = 1 - epsilon + epsilon/A(s) if a* = a\n",
    "                policy(a|s) = epsilon/A(s) if a* != a\n",
    "    \"\"\"\n",
    "    # Keeps track of sum and count of returns for each state\n",
    "    # to calculate an average. We could use an array to save all\n",
    "    # returns (like in the book) but that's memory inefficient.\n",
    "    returns_sum   = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n) \n",
    "    \n",
    "    for i_episode in range(num_episodes): # iterate through episodes\n",
    "        \"\"\" IMPLEMENT HERE \"\"\"\n",
    "        G = 0\n",
    "        state, _ = env.reset()\n",
    "        episode = []\n",
    "        \n",
    "        for t in range(max_steps_per_episode): # for each state appearing episode\n",
    "            \"\"\" IMPLEMENT HERE \"\"\"\n",
    "            probs = policy(state) # get probabilities for each action at a state\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs) # Choose random action using probs\n",
    "            next_state, reward, done, _, info = env.step(action) # use action to set next state, reward, terminal state or not\n",
    "            episode.append((state, action, reward)) # append data to episode\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        for e in episode:\n",
    "            state = e[0]\n",
    "            action = e[1]\n",
    "            reward = e[2]\n",
    "            state_action_pair = (state, action)\n",
    "            first_occurrence = next(\n",
    "                i for i, x in enumerate(episode) if x[0] == state and x[1] == action)\n",
    "\n",
    "            # Calculate Return(G) - sum of cumulative rewards from a state\n",
    "            #G <- return following the first occurrence of (s,a)\n",
    "            G = sum([x[2]*(discount_factor**i) for i, x in enumerate(episode[first_occurrence:])]) # first occurrence of (s,a)\n",
    "            returns_sum[state_action_pair] += G # append G to returns(s,a)\n",
    "            returns_count[state_action_pair] += 1.0 \n",
    "\n",
    "            # Q(s,a) <- average of returns(s,a)\n",
    "            Q[state][action] = returns_sum[state_action_pair] / returns_count[state_action_pair]\n",
    "            # Update Policy\n",
    "                # policy(a|s) = 1 - epsilon + epsilon/A(s) if a* = a\n",
    "                # policy(a|s) = epsilon/A(s) if a* != a\n",
    "            policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "            \n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "m4egtAFA-z22",
    "outputId": "866e90cf-2ca2-4c43-8b9a-242f6a38ac56"
   },
   "outputs": [],
   "source": [
    "Q, policy = mc_control_epsilon_greedy(blackjack_env, num_episodes=500000, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1152
    },
    "colab_type": "code",
    "id": "2swlQVnm-rYP",
    "outputId": "885a0bb8-ef13-4b40-f4f6-856acd8c920a"
   },
   "outputs": [],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "plotting.plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3By1F_amH4v"
   },
   "source": [
    "### Comments on the BlackJack Env\n",
    "\n",
    "Although we have complete knowledge of the environment in the blackjack task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events—in particular, they require the environments dynamics as given by the four-argument function p—and it is not easy to determine this for blackjack. For example, suppose the player’s sum is 14 and he chooses to stick. What is his probability of terminating with a reward of +1 as a function of the dealer’s showing card? All of the probabilities must be computed before DP can be applied, and such computations are often complex and error-prone. In contrast, generating the sample games required by Monte Carlo methods is easy. This is the case surprisingly often; the ability of Monte Carlo methods to work with sample episodes alone can be a significant advantage even when one has complete knowledge of the environment’s dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8mgaNuPhPJk",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## TD Control and Q-Learning\n",
    "Arguably the most famous TD algorithms are SARSA and Q-Learning.\n",
    "We consider the **CliffWorld** for this exercice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 3: complete the following code for Q-Learning (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2LO5257kEdr"
   },
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, epsilon=0.05, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Initialize Q(s,a) arbitrarily\n",
    "    Repeat for each episode:\n",
    "        Initialize S\n",
    "        Repeat for each step in episode:\n",
    "            Choose A from S using policy derived from Q (epsilon-greedy)\n",
    "            Take action A, observe reward and next state\n",
    "            Q(s,a) <- Q(s,a) + alpha(R + max(Q(s', a) - Q(s,a)))\n",
    "            Repeat until S is terminal:\n",
    "                S <- S'\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))  \n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode +1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        # reset environment at the beginning of every episode\n",
    "        \n",
    "        state, _ = env.reset() # Initialize S\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Step\n",
    "            action_probs = policy(state) # get array of possible actions and probabilities\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs) # choose action randomly\n",
    "            next_state, reward, done, _, info = env.step(action) # observe reward and next state based on action\n",
    "          \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Off policy TD\n",
    "            \n",
    "            # max(Q(s', a)), choosing best action\n",
    "            best_action_next_state = argmax(Q[next_state])\n",
    "            \n",
    "            #Q(s,a) <- Q(s,a) + alpha(R + max(Q(s', a) - Q(s,a)))\n",
    "            Q[state][action] = Q[state][action] + alpha * (reward + discount_factor * Q[next_state][best_action_next_state] - Q[state][action])\n",
    "            state = next_state\n",
    "            policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "            if done: # if terminal state\n",
    "                break\n",
    "            \n",
    "               \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1095
    },
    "colab_type": "code",
    "id": "bTfJitmVkFe7",
    "outputId": "597b2c81-0ce8-4c09-b0da-f15f507c61ca"
   },
   "outputs": [],
   "source": [
    "cliffwalking_env = gym.make('CliffWalking-v0')\n",
    "Q, stats_q_learning = q_learning(cliffwalking_env, num_episodes=500)\n",
    "plotting.plot_episode_stats(stats_q_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_values(Q, state_shape=((4, 12))):\n",
    "    \"\"\" helper method to plot a heat map of the states \"\"\"\n",
    "    \n",
    "    values = np.zeros((4 * 12))\n",
    "    max_a  = [0 for _ in range(values.shape[0])]\n",
    "    for key, value in Q.items():\n",
    "        values[key] = max(value)\n",
    "        max_a[key] = int(argmax(value))\n",
    "        \n",
    "    def optimal_move(i, j):\n",
    "        left, right, down, up  = (i, max(j-1, 0)), (i, min(11, j+1)), (min(3, i+1), j), (max(0, i-1), j)\n",
    "        arr = np.array([values[up], values[right], values[down], values[left]])\n",
    "        if i == 2   and j != 11: arr[2] = -9999\n",
    "        if i == 0:  arr[0] = -999\n",
    "        if j == 0:  arr[3] = -999\n",
    "        if j == 11: arr[1] = -999\n",
    "        return argmax(arr)\n",
    "    \n",
    "    # reshape the state-value function\n",
    "    values = np.reshape(values, state_shape)\n",
    "    # plot the state-value function\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow(values)\n",
    "    arrows = ['^', '>', 'v', '<']\n",
    "    index = 0\n",
    "    for (j, i), label in np.ndenumerate(values):\n",
    "        ax.text(i, j, np.round(label, 3), ha='center', va='center', fontsize=12)\n",
    "        if j != 3 or i==0:\n",
    "            ax.text(i, j + 0.4 , arrows[optimal_move(j, i)], ha='center', va='center', fontsize=12, color='red')\n",
    "        index += 1\n",
    "    plt.tick_params(bottom='off', left='off', labelbottom='off', labelleft='off')\n",
    "    plt.title('State-Value Function')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the heatmap, try to follow a greedy policy. Does the trajectory align with optimal path in the previous picture ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "Q-learning is an offline method, since the target update does not depend on the behavior policy (because of the max operator). The online version of Q-Learning is known as SARSA (which stands for State, Action, Reward, State, Action). Notice that in the following pseudocode, the action selected in the target update is the same as the action used in the next timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Question 4: complete the following code for SARSA (25 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(env, num_episodes, discount_factor=1.0, epsilon=0.1, alpha=0.5):\n",
    "    \"\"\"\n",
    "    SARSA algorithm: On-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Initialize Q(s,a) arbitrarily\n",
    "    Repeat for each episode:\n",
    "        Initialize S\n",
    "        Repeat for each step in episode:\n",
    "            Choose A from S using policy derived from Q (epsilon-greedy)\n",
    "            Take action A, observe reward and next state\n",
    "            Choose next action from next state using policy derived from Q (epsilon-greedy)\n",
    "            Q(s,a) <- Q(s,a) + alpha(reward + gamma * Q(s',a') - Q(s,a))\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))  \n",
    "    \n",
    "    for i_episode in range(num_episodes): # Repeat for each episode\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode +1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        \"\"\" IMPLEMENT HERE \"\"\"\n",
    "        state, _ = env.reset() # Initialize S\n",
    "        action_probs = policy(state) # get array of possible actions and probabilities using policy\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs) # # Choose A from S using policy derived from Q (epsilon-greedy)\n",
    "        \n",
    "        for t in itertools.count(): # Repeat for each step in episode\n",
    "            \n",
    "            \"\"\" IMPLEMENT HERE \"\"\"\n",
    "            next_state, reward, done, _, info = env.step(action) # Take action A, observe reward and next state\n",
    "            next_action_probs = policy(next_state) # Choose next action from next state using policy derived from Q (epsilon-greedy)\n",
    "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs) # \n",
    "    \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            #On Policy TD control\n",
    "            \n",
    "            # Q(s,a) <- Q(s,a) + alpha(reward + gamma * Q(s',a') - Q(s,a))\n",
    "            Q[state][action] = Q[state][action] + alpha * (reward + discount_factor * Q[next_state][next_action] - Q[state][action])\n",
    "            state = next_state # S <- S'\n",
    "            action = next_action # A <- A'\n",
    "            if done: # Stop when terminal state reached\n",
    "                break\n",
    "\n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, stats_q_learning = SARSA(cliffwalking_env, num_episodes=500)\n",
    "plotting.plot_episode_stats(stats_q_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "As seen in the slides, you should expect the performance of SARSA to be better than Q-Learning during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (written, 20 points): \n",
    "1) From the heatmap, try to follow a greedy policy. Does the trajectory align with optimal path in the previous picture (by Q-learning)?\n",
    "\n",
    "2) How will SARSA and Q-learning compare if you evaluate the learned policies with $\\epsilon$=0?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MonteCarlo_TD.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
